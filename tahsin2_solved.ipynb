{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Embedding(input_dim=len(tokenizer_code.word_index) + 1, output_dim=64, input_length=padded_code.shape[1]),\n",
    "    keras.layers.LSTM(100),\n",
    "    keras.layers.RepeatVector(padded_summary.shape[1]),\n",
    "    keras.layers.LSTM(100, return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(len(tokenizer_summary.word_index) + 1, activation='softmax'))\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(padded_code, padded_summary, epochs=1000)\n",
    "\n",
    "\n",
    "new_code = [\"def add(a, b): return a + b\"]\n",
    "new_sequence = tokenizer_code.texts_to_sequences(new_code)\n",
    "new_padded = pad_sequences(new_sequence, maxlen=padded_code.shape[1])\n",
    "predicted_sequence = model.predict(new_padded)\n",
    "predicted_summary = [tokenizer_summary.index_word[idx] for idx in tf.argmax(predicted_sequence, axis=-1).numpy()[0] if idx != 0]\n",
    "predicted_summary = \" \".join(predicted_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
